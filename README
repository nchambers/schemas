
Prerequisites
-------------

CAEVO uses the WordNet dictionaries. The dictionary files can be
downloaded from:

    http://wordnetcode.princeton.edu/wn3.1.dict.tar.gz

You also need a jwnl_file_properties.xml file that points to the location of
your downloaded dictionary. The dictionary and this file can be stored anywhere
on your drive. CAEVO looks for an environment variable JWNL that should have the
path to the xml file.


CODE EXECUTION STEPS
--------------------

-----------------------------------------
(1) Pre-Process the text.

The java class AllParser handles parsing, dependencies, events, and coreference. 
The run script runallparser.sh can be used.

./runallparser.sh -output <output-directory> -input text <input-directory>

This requires the parser's serialized file: englishPCFG.ser.gz 
AllParser searches a couple paths for that file, including the current directory. You could use a symlink to point to it.

* The "-input text" flag defines the type of input. This version has not been extensively tested. Check TextHandler.java if fixes need to be made. It treats each file as a single document.



-----------------------------------------
(2) Compute Inverse Document Frequencies.

The class CalculateIDF handles word counting and IDF computation.
It inputs a directory or single file of .parse files (created from Step 1), and 
creates two .idf files in a specified output directory. A run script is provided:

./runidf.sh -outdir domain/ <parser-output-dir>

You must create the output directory before running it.


-----------------------------------------
(3) Create the domain's word/dep counts.

./runverbdep.sh -output domain/ -deps <parser-output-dir>/file.deps -events <parser-output-dir>/file.events -parsed <parser-output-dir>/file.parse -ner <parser-output-dir>/file.ner

Creates a file in depcounts/ that contains all words with their typed dependencies, and the observed counts for these. For instance, "v-arrest" will have its own line with a bunch of counts of subjects, objects, and other relations. This will be used as a measure of a word's "importance" to your dataset.


-----------------------------------------
(4) Create the domain's pair counts.

./runcountpairs.sh -type base -dependents -fullprep -output domain/ -idf domain/tokens-lemmas.idf -deps domain/<depsfile> -parsed domain/<parsefile>
** rename output file to "token-pairs-base-lemmas-govdep-distlog4.counts"

./runcountpairs.sh -type vbnn -dependents -fullprep -nodistance -withcoref -output domain/ -idf domain/tokens-lemmas.idf -deps domain/<depsfile> -parsed domain/<parsefile> -events domain/<eventsfile>
** rename output file to "token-pairs-vbnn-coref-fullprep.counts"


-----------------------------------------
(5) Create the domain's argument counts.

./runargs.sh -idf domain/tokens-lemmas.idf -output <out-dir> -events <domain-events> -deps <domain-deps> -parsed <domain-parses> -ner <domain-ner>

The important output file here is argcounts-verbs.arg. Don't worry about argcounts-pairs.arg as it is not used.


-----------------------------------------
(6) LEARN TEMPLATES

./runlabeldoc.sh all vbnom 40

The above clusters all verbs and nouns, and limits cluster size to 30 at most. Agglomerative clustering runs until one of the clusters reaches that size, then it stops. Lots of debug output will print. The main clustering code is in ClusterMUC.hierarchicalCluster().

Look for "Finished clustering.". The following lines will have the learned Frames printed out, one per line with a score. Many of the frames may seem unclear as to why they were learned, but you should see lots of good clusters with related events in them. If everything looks random, then something went wrong.

Look for "Going to induce slots...". This section begins the final phase that will try to learn the entity slots for the frames/templates.
